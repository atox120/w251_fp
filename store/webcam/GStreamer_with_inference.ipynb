{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1845d373-a8e4-45fa-a180-1029e9cdbe6b",
   "metadata": {},
   "source": [
    "## GStreamer Basics - basic pipeline read buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fa6d2-c1c4-457a-81fb-7a4f9b054b36",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "Code adapted from:\n",
    "https://paulbridger.com/posts/video-analytics-pytorch-pipeline/\n",
    "\n",
    "\n",
    "Gstreamer References:\n",
    "https://developer.download.nvidia.cn/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf\n",
    "\n",
    "DeepStream:\n",
    "https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60460e2-5a1f-46cb-b4bd-7126c9b556eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(os.path.abspath(\"../Video-Swin-Transformer\"))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "import mmcv\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.fileio.io import file_handlers\n",
    "from mmcv.runner.fp16_utils import wrap_fp16_model\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
    "\n",
    "from mmaction.models import build_model\n",
    "from mmaction.utils import register_module_hooks\n",
    "\n",
    "import contextlib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using: {device}')\n",
    "\n",
    "# Setup the cofiguration and data file\n",
    "config_file = '../configs/bsl_config.py'\n",
    "input_video_path = '../notebooks/source_video.mp4'\n",
    "check_point_file = '../configs/best_model.pth'\n",
    "frames_processed = 0\n",
    "pixel_bytes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf66eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream for my device - no display on screen  - extra nvvidconv needed since \n",
    "# gst buffer cannot read from memory otherwise. \n",
    "v7 = \"\"\"\n",
    "v4l2src device=/dev/video0 ! nvvidconv ! \\\n",
    "video/x-raw(memory:NVMM),framerate=(fraction)30/1,width=320,height=240 ! \\\n",
    "nvvidconv top=0 bottom=240 left = 90 right=320 ! \\\n",
    "video/x-raw(memory:NVMM),width=224,height=224 ! nvvidconv ! video/x-raw,format=RGBA ! \\\n",
    "fakesink name=webcam_stream\n",
    "\"\"\"\n",
    "\n",
    "v8 = \"\"\"\n",
    "v4l2src num-buffers=128 device=/dev/video0 ! nvvidconv ! \\\n",
    "video/x-raw(memory:NVMM),framerate=(fraction)30/1,width=320,height=240 ! \\\n",
    "nvvidconv top=0 bottom=240 left = 90 right=320 ! \\\n",
    "video/x-raw(memory:NVMM),format=RGBA,width=224,height=224 ! nvvidconv ! video/x-raw ! \\\n",
    "queue ! tee name=t t. ! queue ! fakesink name=webcam_stream sync=true t. ! \\\n",
    "queue ! nvvidconv ! nvegltransform ! nveglglessink sync=true\n",
    "\"\"\"\n",
    "\n",
    "stream_string = v8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2b770-9700-45df-a3c2-d7c835416806",
   "metadata": {},
   "source": [
    "### Do inference every 32 frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff6c26f-74e8-457b-b0e7-71aea8533ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config_file, check_point_file, device='cuda:0'):\n",
    "# Create the configuration from the file \n",
    "# Customization for training the BSL data set\n",
    "    cfg = Config.fromfile(config_file)\n",
    "    cfg.model.cls_head.num_classes = 5\n",
    "    cfg.data.test.test_mode = True\n",
    "\n",
    "    # The flag is used to register module's hooks\n",
    "    cfg.setdefault('module_hooks', [])\n",
    "\n",
    "    # remove redundant pretrain steps for testing\n",
    "    turn_off_pretrained(cfg.model)\n",
    "\n",
    "    # build the model and load checkpoint\n",
    "    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "    if len(cfg.module_hooks) > 0:\n",
    "        register_module_hooks(model, cfg.module_hooks)\n",
    "\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        wrap_fp16_model(model)\n",
    "\n",
    "    load_checkpoint(model, check_point_file, map_location=device)\n",
    "        \n",
    "    return model # model = MMDataParallel(model, device_ids=[0])\n",
    "\n",
    "def turn_off_pretrained(cfg):\n",
    "    # recursively find all pretrained in the model config,\n",
    "    # and set them None to avoid redundant pretrain steps for testing\n",
    "    if 'pretrained' in cfg:\n",
    "        cfg.pretrained = None\n",
    "\n",
    "    # recursively turn off pretrained value\n",
    "    for sub_cfg in cfg.values():\n",
    "        if isinstance(sub_cfg, dict):\n",
    "            turn_off_pretrained(sub_cfg)\n",
    "\n",
    "def preprocess_batch(image_batch):\n",
    "    \n",
    "    preproc_time = time.time()\n",
    "    mu_vec = np.reshape([123.675, 116.28, 103.53], (1, 1, 3)) \n",
    "    sigma_vec = np.reshape([58.395, 57.12, 57.375], (1, 1, 3))\n",
    "    \n",
    "    image_batch = np.divide((image_batch - mu_vec), sigma_vec)\n",
    "    \n",
    "    image_batch = image_batch.transpose(3,0,1,2)\n",
    "    \n",
    "    image_batch = image_batch[np.newaxis, np.newaxis, :, :, :, :]\n",
    "    \n",
    "    print(f\"Preprocessing duration: {time.time()-preproc_time:.4f} sec\")\n",
    "    return image_batch\n",
    "\n",
    "def preprocess_tensor_batch(image_batch):\n",
    "    \n",
    "    preproc_time = time.time()\n",
    "    mu_vec = torch.from_numpy(np.reshape([123.675, 116.28, 103.53], (1, 1, 3))) \n",
    "    sigma_vec = torch.from_numpy(np.reshape([58.395, 57.12, 57.375], (1, 1, 3)))\n",
    "    \n",
    "    image_batch = torch.div((image_batch - mu_vec), sigma_vec)\n",
    "    \n",
    "    image_batch = image_batch.transpose(3,0,1,2)\n",
    "    \n",
    "    image_batch = image_batch[None, None, :, :, :, :]\n",
    "    \n",
    "    print(f\"Preprocessing duration: {time.time()-preproc_time:.4f} sec\")\n",
    "    return image_batch\n",
    "\n",
    "def interpret(outputs):\n",
    "    text_form = ('all_done', 'water', 'poop', 'dad', 'mom')\n",
    "    \n",
    "    index = [np.argmax(x) for x in outputs][0]\n",
    "    predicted_prob = outputs[0][index]\n",
    "    action = text_form[index]\n",
    "    \n",
    "    if predicted_prob < 0.1:\n",
    "        action = '..'  \n",
    "    else:\n",
    "        # Add the probability of that action and inference number\n",
    "        action = f'{action} - prob {predicted_prob:.2f}'\n",
    "    print(f'Action: {action}')\n",
    "\n",
    "def on_frame_probe(pad, info):\n",
    "    \n",
    "    global t_start, frames_processed, image_batch\n",
    "    t_start = time.time()\n",
    "    \n",
    "    buffer = info.get_buffer()\n",
    "    print(f'[{buffer.pts / Gst.SECOND:6.2f}]', end='\\x1b[2K\\r')\n",
    "\n",
    "    image_tensor = buffer_to_image_tensor(buffer, pad.get_current_caps())\n",
    "    image_batch.append(image_tensor)\n",
    "\n",
    "    # If batch not yet full, keep accumulating frames.\n",
    "    if len(image_batch) < batch_size:\n",
    "        return Gst.PadProbeReturn.OK\n",
    "    \n",
    "    #Do preprocessing on all 32 images at once. \n",
    "    image_batch = preprocess_batch(np.stack(image_batch, axis = 0))\n",
    "    frames_processed += image_batch.shape[3]\n",
    "\n",
    "    batch_tensors = torch.from_numpy(image_batch).to(torch.float32)\n",
    "    with torch.no_grad():\n",
    "        inf_timer = time.time()\n",
    "        outputs = infer(model, {\"imgs\": batch_tensors,\"label\":0})\n",
    "        interpret(outputs)\n",
    "        image_batch = []\n",
    "        print(f\"Inference time: {time.time() - inf_timer:.4f} sec\")\n",
    "    return Gst.PadProbeReturn.OK\n",
    "\n",
    "def buffer_to_image_tensor(buffer, caps):\n",
    "    \n",
    "    caps_structure = caps.get_structure(0)\n",
    "    height = caps_structure.get_value('height')  \n",
    "    width = caps_structure.get_value('width')\n",
    "\n",
    "    is_mapped, map_info = buffer.map(Gst.MapFlags.READ)\n",
    "    if is_mapped:\n",
    "        try:\n",
    "            # map buffer to numpy \n",
    "            image_array = np.ndarray(\n",
    "                (height, width, pixel_bytes),\n",
    "                dtype=np.uint8,\n",
    "                buffer=map_info.data\n",
    "            )\n",
    "            # Return a copy of that array as a numpy array. \n",
    "            return image_array[:,:,:3].copy() \n",
    "        finally:\n",
    "            #Clean up the buffer mapping\n",
    "            buffer.unmap(map_info)\n",
    "\n",
    "def infer(model, batch):\n",
    "    \"\"\"Test model with a single gpu.\n",
    "    This method tests model with a single gpu and displays test progress bar.\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        data_loader (nn.Dataloader): Pytorch data loader.\n",
    "    Returns:\n",
    "        list: The prediction results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    result = model(return_loss=False, **batch)\n",
    "    results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5861fbc-ef84-4c8d-96ec-b1b5c7866ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants loaded in 0.0006 seconds\n",
      "../Video-Swin-Transformer/configs/_base_/models/swin/swin_tiny.py\n",
      "../Video-Swin-Transformer/configs/_base_/default_runtime.py\n",
      "load checkpoint from local path: ../configs/best_model.pth\n",
      "Model loaded in 18.3397 seconds\n",
      "Pipeline launched\n",
      "Preprocessing duration: 2.1786 sec\n",
      "Action: dad - prob 0.32\n",
      "Inference time: 31.2310 sec\n",
      "Preprocessing duration: 0.4571 sec\n",
      "Action: dad - prob 0.32\n",
      "Inference time: 23.7572 sec\n",
      "Preprocessing duration: 0.2045 sec\n",
      "Action: dad - prob 0.33\n",
      "Inference time: 22.3872 sec\n",
      "Preprocessing duration: 0.2006 sec\n",
      "Action: dad - prob 0.32\n",
      "Inference time: 22.3723 sec\n",
      "pipeline0: [eos] \n",
      "Writing graph...\n",
      "FPS: 5.45\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Pipeline\n",
    "Gst.init()\n",
    "\n",
    "v_timer = time.time()\n",
    "frames_processed = 0\n",
    "image_batch = []\n",
    "batch_size = 32\n",
    "batch_dim = (32,224,224)\n",
    "\n",
    "#Load initial Constants\n",
    "#global mu_vec, sigma_vec\n",
    "#mu_vec, sigma_vec = instantiate_vectors(batch_dim)\n",
    "\n",
    "print(f'Constants loaded in {time.time()-v_timer:.4f} seconds') \n",
    "\n",
    "m_timer = time.time() \n",
    "# Instantiate Model\n",
    "model = get_model(config_file, check_point_file, device=device).eval()\n",
    "print(f'Model loaded in {time.time()-m_timer:.4f} seconds') \n",
    "\n",
    "# Launch Pipeline\n",
    "pipeline = Gst.parse_launch(f\"{stream_string}\")\n",
    "print(f'Pipeline launched') \n",
    "\n",
    "# Probe read buffer and apply callback 'on_frame_probe'\n",
    "pipeline.get_by_name('webcam_stream').get_static_pad('sink').add_probe(\n",
    "    Gst.PadProbeType.BUFFER,\n",
    "    on_frame_probe\n",
    ")\n",
    "\n",
    "# Set Pipeline to PLAYING\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "# Scan the pipeline bus for errors and other messages.\n",
    "try:\n",
    "    while True:\n",
    "        msg = pipeline.get_bus().timed_pop_filtered(\n",
    "            Gst.SECOND,\n",
    "            Gst.MessageType.EOS | Gst.MessageType.ERROR\n",
    "        )\n",
    "        if msg:\n",
    "            text = msg.get_structure().to_string() if msg.get_structure() else ''\n",
    "            msg_type = Gst.message_type_get_name(msg.type)\n",
    "            print(f'{msg.src.name}: [{msg_type}] {text}')\n",
    "            break\n",
    "            \n",
    "# Break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('KeyboardInterrupt exception is caught')\n",
    "    \n",
    "# Write pipeline Graph\n",
    "finally:\n",
    "    print('Writing graph...')\n",
    "    open(f'logs/logs.txt', 'w', encoding=\"utf8\").write(\n",
    "        Gst.debug_bin_to_dot_data(\n",
    "            pipeline, Gst.DebugGraphDetails.ALL\n",
    "        )\n",
    "    )\n",
    "    pipeline.set_state(Gst.State.NULL)\n",
    "t_finish = time.time()\n",
    "print(f'FPS: {frames_processed / (t_finish - t_start):.2f}')\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f94cc9-004a-4f7f-a726-2f2cf32ff418",
   "metadata": {},
   "source": [
    "### Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29338b-7bab-40c8-81b7-3107612ee4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Graveyard...\n",
    "for words in [\"these\", \"are\", \"my\", \"words\", \"!\"]:\n",
    "    print(words, end='\\x1b[2K\\r')\n",
    "    time.sleep(1)\n",
    "    print(\" \"*20, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe894a8-31ff-4546-be91-cc3f70a41bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config_file, check_point_file, device='cuda:0'):\n",
    "# Create the configuration from the file \n",
    "# Customization for training the BSL data set\n",
    "    cfg = Config.fromfile(config_file)\n",
    "    cfg.model.cls_head.num_classes = 5\n",
    "    cfg.data.test.test_mode = True\n",
    "\n",
    "    # The flag is used to register module's hooks\n",
    "    cfg.setdefault('module_hooks', [])\n",
    "\n",
    "    # remove redundant pretrain steps for testing\n",
    "    turn_off_pretrained(cfg.model)\n",
    "\n",
    "    # build the model and load checkpoint\n",
    "    model = build_model(cfg.model, train_cfg=None, test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "    if len(cfg.module_hooks) > 0:\n",
    "        register_module_hooks(model, cfg.module_hooks)\n",
    "\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        wrap_fp16_model(model)\n",
    "\n",
    "    load_checkpoint(model, check_point_file, map_location=device)\n",
    "        \n",
    "    return model # model = MMDataParallel(model, device_ids=[0])\n",
    "\n",
    "def turn_off_pretrained(cfg):\n",
    "    # recursively find all pretrained in the model config,\n",
    "    # and set them None to avoid redundant pretrain steps for testing\n",
    "    if 'pretrained' in cfg:\n",
    "        cfg.pretrained = None\n",
    "\n",
    "    # recursively turn off pretrained value\n",
    "    for sub_cfg in cfg.values():\n",
    "        if isinstance(sub_cfg, dict):\n",
    "            turn_off_pretrained(sub_cfg)\n",
    "\n",
    "def single_gpu_predictor(model, data_loader):\n",
    "    \"\"\"Test model with a single gpu.\n",
    "    This method tests model with a single gpu and displays test progress bar.\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        data_loader (nn.Dataloader): Pytorch data loader.\n",
    "    Returns:\n",
    "        list: The prediction results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    #for data in data_loader:\n",
    "    with torch.no_grad():\n",
    "        result = model(return_loss=False, **data)\n",
    "    results.extend(result)\n",
    "    return results\n",
    "\n",
    "def normalize(frame):\n",
    "    mean = np.reshape([123.675, 116.28, 103.53], (1, 1, 3)) \n",
    "    std = np.reshape([58.395, 57.12, 57.375], (1, 1, 3))\n",
    "    \n",
    "    # Mean normalize then stdev normalize\n",
    "    frame = (frame - mean) / std   \n",
    "    return frame\n",
    "\n",
    "\n",
    "def interpret(index):\n",
    "    text_form = ['all_done', 'water', 'poop', 'dad', 'mom']\n",
    "\n",
    "    return text_form[index]\n",
    "\n",
    "\n",
    "def on_frame_probe(pad, info):\n",
    "    \n",
    "    global t_start, frames_processed\n",
    "    t_start = time.time()\n",
    "    \n",
    "    buffer = info.get_buffer()\n",
    "    print(f'[{buffer.pts / Gst.SECOND:6.2f}]')\n",
    "\n",
    "    image_tensor = buffer_to_image_tensor(buffer, pad.get_current_caps())\n",
    "    image_batch = image_tensor.unsqueeze(0).to(device)\n",
    "    frames_processed += image_batch.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    #detections = detector(image_batch)[0]\n",
    "        #predictions = model(image_batch)[0]\n",
    "        print('Inference Call Placeholder')\n",
    "    return Gst.PadProbeReturn.OK\n",
    "\n",
    "def buffer_to_image_tensor(buffer, caps):\n",
    "    \n",
    "    caps_structure = caps.get_structure(0)\n",
    "    height = caps_structure.get_value('height')  \n",
    "    width = caps_structure.get_value('width')\n",
    "\n",
    "    is_mapped, map_info = buffer.map(Gst.MapFlags.READ)\n",
    "    if is_mapped:\n",
    "        try:\n",
    "            image_array = np.ndarray(\n",
    "                (height, width, pixel_bytes),\n",
    "                dtype=np.uint8,\n",
    "                buffer=map_info.data\n",
    "            ).copy() # extend array lifetime beyond subsequent unmap\n",
    "            return preprocess(image_array[:,:,:3]) # RGBA -> RGB\n",
    "        finally:\n",
    "            \n",
    "            #Clean up the buffer mapping\n",
    "            buffer.unmap(map_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f3fa1-1ddc-4c78-a2b1-262e831f4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline\n",
    "Gst.init()\n",
    "\n",
    "# Define Preprocess function. \n",
    "preprocess = torchvision.transforms.ToTensor()\n",
    "m_timer = time.time()\n",
    "# Instantiate Model\n",
    "model = get_model(config_file, check_point_file, device=device).eval()\n",
    "print(f'Model loaded in {time.time()-m_timer:.2f} seconds') \n",
    "\n",
    "# Launch Pipeline\n",
    "pipeline = Gst.parse_launch(f\"{stream_string}\")\n",
    "\n",
    "# Probe read buffer and apply callback 'on_frame_probe'\n",
    "pipeline.get_by_name('webcam_stream').get_static_pad('sink').add_probe(\n",
    "    Gst.PadProbeType.BUFFER,\n",
    "    on_frame_probe\n",
    ")\n",
    "\n",
    "# Set Pipeline to PLAYING\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "# Scan the pipeline bus for errors and other messages.\n",
    "try:\n",
    "    while True:\n",
    "        msg = pipeline.get_bus().timed_pop_filtered(\n",
    "            Gst.SECOND,\n",
    "            Gst.MessageType.EOS | Gst.MessageType.ERROR\n",
    "        )\n",
    "        if msg:\n",
    "            text = msg.get_structure().to_string() if msg.get_structure() else ''\n",
    "            msg_type = Gst.message_type_get_name(msg.type)\n",
    "            print(f'{msg.src.name}: [{msg_type}] {text}')\n",
    "            break\n",
    "            \n",
    "# Break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('KeyboardInterrupt exception is caught')\n",
    "    \n",
    "# Write pipeline Graph\n",
    "finally:\n",
    "    print('Writing graph...')\n",
    "    open(f'logs/logs.txt', 'w', encoding=\"utf8\").write(\n",
    "        Gst.debug_bin_to_dot_data(\n",
    "            pipeline, Gst.DebugGraphDetails.ALL\n",
    "        )\n",
    "    )\n",
    "    pipeline.set_state(Gst.State.NULL)\n",
    "t_finish = time.time()\n",
    "print(f'FPS: {frames_processed / (t_finish - t_start):.2f}')\n",
    "print('done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
